# DYPS

## 第一部分

有效的参数共享仍然存在两个主要挑战：（1）在强化学习训练过程中，智能体的行为发生显着变化，限制了基于训练前确定的固定角色划分的组参数共享的性能； (2)智能体的行为形成复杂的动作轨迹，其角色特征是隐含的，增加了训练过程中动态调整智能体角色划分的难度。 在本文中，我们提出动态参数共享（DyPS）来解决上述挑战。 

### 第一个挑战：

**智能体的行为在训练过程中显著变化，限制了基于训练前确定的固定角色划分的组参数共享的性能**
解释：

- 在强化学习中，智能体的行为是通过与环境互动不断调整和优化的。初期，智能体的行为可能很随机，并没有明确的角色或策略分工。
- 随着训练的进行，智能体逐渐学会如何更好地完成任务，其行为会变得越来越复杂、精细，甚至会根据环境的不同而发生变化。
- 如果我们在训练开始前就确定好不同智能体之间的角色划分（例如，有些智能体负责探索、一些智能体负责执行策略），然后让这些智能体在这些固定角色之间共享参数，这种方法可能会遇到问题。因为训练过程中，智能体的行为会改变，而固定的角色划分可能无法适应这些行为的动态变化，导致共享的参数无法充分优化，从而影响训练效果。

### 第二个挑战：

**智能体的行为形成复杂的动作轨迹，其角色特征是隐含的，增加了训练过程中动态调整智能体角色划分的难度**
解释：

- 在强化学习中，智能体在环境中的行为表现为一系列动作和决策，形成了复杂的轨迹。每个动作和决策都依赖于环境状态，智能体的决策也在不断变化和适应。
- 这些行为轨迹背后的角色特征往往并不是显而易见的，而是隐含在智能体的决策过程中。例如，一个智能体可能在某些情况下表现出探索性行为，而在其他情况下表现出执行性行为，但这些角色并不总是明确的。
- 在这种情况下，智能体的角色特征并不是事先能够简单定义的，而是需要根据训练过程中智能体行为的变化动态地调整。传统的基于固定角色的参数共享方法在面对这种动态调整时会变得困难，因为智能体在不同训练阶段的角色表现和需求可能会变化。

具体来说，DyPS 包括自我监督的角色建模部分和分层 MARL 部分。 前一部分采用变分长短期记忆（VLSTM）[15]和条件变分自动编码器（CVAE）[38]分别从动作轨迹中描述每个智能体和每个组的行为特征。 后者由两个层次组成，即组选择模块和基于组的资源分配模块，其中组选择模块根据VLSTM和CVAE提取的行为特征，根据角色对代理进行动态分组，以及基于组的资源分配模块。 分配模块学习多智能体资源分配策略，并在同一组内的智能体之间共享参数。 这两个模块都是根据代理环境交互中的反馈通过 Actor-Critic (AC) [24] 方式进行训练。 我们通过 4 个代表性资源分配场景的 7 个实验评估了我们的框架，广泛的结果表明其卓越的性能，超过最先进的基线方法高达 31%。 此外，我们的框架可以轻松地与多种典型的 MARL 算法结合，并提高它们在资源分配问题上的性能。

### 贡献

• 我们为大规模资源分配问题设计了一个动态参数共享MARL 框架。 该框架通过环境反馈进行训练，环境反馈会根据智能体在强化学习训练期间不断变化的角色**对智能体进行动态分组**。 然后，**同一组内的代理共享参数**，降低计算复杂性，同时**保持代理角色的最新建模**。

 • 我们采用**VLSTM 和CVAE **通过自我监督学习从代理的行动轨迹中提取行为特征。 因此，我们可以根据智能体的行为特征来识别其角色，然后在强化学习训练期间动态调整分组。 

• 对各种场景的大量实验表明，DyPS 显着优于最先进的基线方法，在资源分配任务上的改进高达 31%。 此外，DyPS可以很容易地与多种MARL算法结合，提高其资源分配的性能。

## 第二部分

### 问题定义

论文研究了一个**目标城市区域**，这个区域被划分成$K$ 个**互不重叠的均匀网格**（grids）。资源和需求在这些网格中分布，并且研究的是跨越 $T$ 个时间步长的问题（即任务持续 $T$ 个时间单位）。

主要的定义如下：

**Definition 1: 资源 (Resource)**

- **资源的定义：** 在某一时间步 $t$，可用资源的总数用$N_t$ 表示。
- 这些资源分布在 $K $个网格中，第$k$ 个网格中在时间步$t $的资源数量用 $N_t^k$ 表示。
- **总资源约束：** $\sum_k N_t^k = N_t$。
  这意味着在每个时间步 $t$，网格中所有资源的总和等于总资源量$N_t$​。

**Definition 2: 需求 (Demand)**

- **需求的定义：** 在某一时间步 $t$，第 $k$ 个网格的资源需求量用 $D_t^k$ 表示。

- **总需求量：** $\sum_k D_t^k$。

- 总需求量和总资源量 $N_t$

   可以是：

  - $\sum_k D_t^k > N_t$：资源稀缺。
  - $\sum_k D_t^k < N_t$：资源富余。

**Definition 3: 时空资源分配 (Spatio-Temporal Resource Allocation)**

- 目标区域在 $T$ 个时间步长内具有资源和需求。
- **资源分配策略：** 是在网格之间移动资源，以满足各网格的需求。
- **资源移动量：** 从网格 $i$ 到网格 $j $在时间$t $移动的资源数量用 $M_t^{ij}$表示。
- 约束条件：
  - 资源的移动需要遵守任务的限制条件，例如资源的最大移动量、时间窗口等。

1. 我们研究的是一个城市区域的资源和需求分布问题。
2. 问题的核心是如何在满足需求的前提下，最优地调度资源，并最大化整体效益。
3. 研究需要考虑的因素包括资源的空间分布、时间变化、资源移动的限制等。

### 多智能体马尔科夫决策（MAMDP）

**MAMDP 的定义：**

一个 MAMDP 可以用六元组表示：$(n, S, A, P, R, \gamma)$，其中：

1. **$n$：智能体数量**

   - 这是指参与分配资源的智能体总数（例如，每个网格可能由一个智能体控制）。

2. **$S$：全局状态空间**

   - 全局状态$s $是所有智能体的局部状态$(s_1, \cdots, s_n)$的组合。

   - 每个局部状态 $s_i$

      可能包括：

     - 当前网格的资源分布情况。
     - 需求分布。
     - 环境的其他相关特征。

3. **$A$：联合动作空间**

   - 全局的联合动作 $a$ 是所有智能体局部动作的组合 $(a_1, \cdots, a_n)$。
   - 每个局部动作$a_i$是智能体$i $在其负责的区域内的资源分配或移动决策。

4. **$P$：状态转移概率函数**

   - $P(s, a, s')$ 表示从状态 $s $在联合动作 $a$ 下转移到后续状态 $s′ $的概率。
   - 状态转移捕捉了资源移动和需求变化的动态过程。

5. **$R$：全局奖励函数**

   - 奖励是所有智能体局部奖励$r_i$的加总：$R_t(s_t, a_t) = \sum_{i=1}^n r_i(s_t, a_t)$
   - 奖励可以衡量当前动作对满足需求和分配资源的贡献。

6. **$\gamma$：折扣因子**

   - 用于控制未来奖励的权重（$\gamma \in [0, 1]$）。
   - 折扣因子越小，智能体越关注即时奖励，而非长远效益。

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\076544cb5f69d3a8988abe19b3358a66.png" alt="076544cb5f69d3a8988abe19b3358a66" style="zoom:80%;" />





Actor-Critic (AC)方法是MAMDP的典型解决方案

MAMDP问题：多智能体马尔科夫决策过程。

演员——评论家算法：包含两个量：a critic $𝑉_{𝜋𝜃}$ and an actor 𝜋𝜃 . 

在训练过程中，通过对当前状态的值进行估计，批评家 𝑉𝜋𝜃起到了基于值的方法的作用。其目的是最小化TD误差𝛿𝑡，以精确估计当前状态的值:

 **时间差分（TD）误差：**

$𝛿_𝑡 = (𝛾𝑟(𝑠_𝑡, 𝑎_𝑡 ) + 𝑉_{𝜋_𝜃} (𝑠_{𝑡+1}) − 𝑉_{𝜋_𝜃} (𝑠_𝑡 )^2$

- $𝑟(𝑠_𝑡, 𝑎_𝑡 )$:当前状态 $s_t$ 下，采取动作$a_t$获得的即时奖励。
- $𝑉_{𝜋_𝜃} (𝑠_𝑡 )$:当前状态的估计价值。
- $𝛾 $  :折扣因子，衡量奖励的重要性。

通过 TD 误差，Critic 会不断比较“目前的打分”与“实际表现（即时奖励和下一步未来收益）”之间的差异，并调整打分方法，使其更加合理。**这里的公式与经典TD误差公式还有所不同**。

行动者(actor)通过与环境相互作用并根据当前政策产生动作，从而发挥基于策略方法的作用。它利用了一个**优势函数**

$𝐴_{𝜋_𝜃}(𝑠_𝑡, 𝑎_𝑡 ) = 𝑟(𝑠_𝑡, 𝑎_𝑡 ) + 𝑉_{𝜋_𝜃}(𝑠_{𝑡+1}) − 𝑉_{𝜋_𝜃}(𝑠_𝑡 )$,(2)

- Advantage 函数衡量某个动作 $a_t$ 相较于当前策略的平均表现 $V_{\pi_\theta}(s_t)$的优劣性。
- 当$A_{\pi_\theta}(s_t, a_t) > 0$ 时，表示动作 $a_t$的表现优于策略的期望值，策略应该更倾向于选择该动作。
- Advantage 函数使得策略更新更加稳定，因为它只关注动作的“相对表现”。

**𝜋𝜃 通过以下式子进行更新：**

$∇𝐽(𝜃) = E[∇𝜃𝑙𝑜𝑔𝜋_𝜃 (𝑠_𝑡, 𝑎_𝑡 )𝐴_{𝜋_𝜃}(𝑠_𝑡, 𝑎_𝑡 )]$

- $\nabla_\theta \log \pi_\theta(s_t, a_t)$：表示策略 $\pi_\theta$ 的梯度，用于指导策略参数$\theta$ 的优化方向。
- $A_{\pi_\theta}(s_t, a_t)$：Advantage 函数，用于衡量当前动作 $a_t$ 的优劣性。
- **意义：** 策略通过梯度上升优化，使得选择表现优的动作（即 $A_{\pi_\theta}(s_t, a_t) > 0$）的概率增大，而不佳的动作概率减小。

## 第三部分

### 系统概述

图2给出了dyps体系结构的概述。DyPS由三个模块组成:自我监督角色建模、小组选择和基于小组的资源分配。对于时空资源分配问题，智能体表现出相似性和差异性。我们使用VLSTM来捕获单个智能体的特征，并使用CVAE来编码不同组的功能。分组选择模块基于VLSTM和CVAE提取的行为特征，对agent进行动态分组。基于组的资源分配模块包含多个资源分配策略网络，每个网络对应一个特定的资源分配模式。分层决策结构为针对不同资源的决策提供了丰富的行为模式。同时，通过行为模式共享实现参数共享，降低训练成本。

### 自我监督角色建模

使用条件变分自编码器（Conditional Variational Autoencoder, CVAE）来实现群体角色建模

为了有效地将代理分组并确定哪些代理共享参数，必须对组和单个代理进行角色建模。我们分别采用两种不同的自监督方法对群体角色和个体代理角色建模，具体如下:

#### CVAE代表群体角色建模

同一组内的代理共享同一策略网络。对每个组的角色进行建模对于有效地聚类代理至关重要。已有研究表明策略网络的行为模式可以通过从agent与环境的交互中得到的状态-动作对来建模。考虑到在某些状态下，不同的政策网络可能做出相同的决策，因此相同的状态-行动对可能对应于几个政策网络。因此，我们使用CVAE对不同策略网络的行为模式进行编码，如图3所示。CVAE同时学习一个概率编码器$q(z^{(g)}[m]I^{(g)}[m];\theta^e)$和一个概率解码器$p(a_t|z^{(g)}[m],s_t;\theta^d)$。式中，$I^{(g)}[m]$是表示对资源分配的识别，策略网络m,$s_t$和$a_t$是资源的状态和策略网络输出的动作，$z^{(g)}[m]$是组m的角色编码。与经典的自编码器结构不同，$s_t$绕过编码器，只能由解码器接收。

基于这种设计，$z^{(𝑔)}[𝑚]$只包含策略本身的信息。我们设置先验$𝑝(z^{(𝑔)}[𝑚];\theta^p)$是标准的多元高斯分布，CVAE的学习就是像VAE一样**最大化证据下界ELBO**，后一项是Kullback-Leibler 散度，||表示对两个概率的比较。

CVAE是VAE的扩展版，区别是CVAE引入了条件变量，能够根据额外信息控制生成数据的特性。

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\4128633b544b4907fa7425aeb0a1ba41.png" alt="4128633b544b4907fa7425aeb0a1ba41" style="zoom:75%;" />



#### 用于Agent角色建模的VLSTM

**变分长短期记忆网络（VLSTM）** 的方法对代理的角色进行建模

在本节中，我们使用VLSTM从轨迹历史中建模代理的角色。最近的研究[11,20,29]表明，代理人的轨迹包含时空信息。提取这些信息可以帮助组选择代理精确地为代理选择组。因此，我们利用VLSTM[15]从资源的轨迹历史中提取时空特征，将变分自编码器(VAE)[23]与LSTM相结合，增强其在动态环境(如现实世界的网约车)[15]中的鲁棒性[8]。

如图4所示，VLSTM模型包含一个推理模型和一个生成模型。agent𝑛的轨迹历史记为$ T^{(𝑎)}_t[𝑛]= { 𝑠_0，𝑎_0，𝑟_1，𝑠_1，，···，𝑠_{𝑡−1}，𝑎_{𝑡−1}，𝑟_𝑡 }$。在下面的公式中，为简单起见，我们省略了代理识别。例如，我们使用$T_𝑡$代替$T^{(𝑎)}_𝑡[𝑛]$。VLSTM模块可以学习用随机潜变量$z_𝑡$编码复杂的序列特征$T_𝑡$。生成模型$𝑝_{\theta}$预测给定其内部状态$h_{𝑡−1}$的状态-奖励对$𝑒_{𝑡+1} =(𝑠_{𝑡+1}，𝑟_{𝑡+1})$，如图4(c)所示。

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\00dafca35eeaf48a7433b5e13d376d57.png" alt="00dafca35eeaf48a7433b5e13d376d57" style="zoom:75%;" />

从历史状态$h_{t-1}$ 中预测潜在变量 $z_t$ 的均值和方差（5）

基于$z_t$ 和 $h_{t-1}$，预测状态-奖励对的均值和方差(7)

其中$𝑓_𝑝$是参数化前馈神经网络，$ℎ_{𝑡−1}$是LSTM的隐藏状态变量，其包含历史时空信息，该历史时空信息可以用该公式递归地更新，如图4(A)所示<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\56411610004c1f2d1d4e771418abde41.png" alt="56411610004c1f2d1d4e771418abde41" style="zoom:75%;" />

隐藏状态 $h_t$ 由 LSTM 层更新(9),其中$𝑓^𝑒$是LSTM层。

如图4(b)所示，VLSTM的推理模型$𝑞$近似于潜变量$z_𝑡$由给定状态$𝑠_𝑡$和隐状态变量$h_{𝑡−1}$推断.

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\14fad07952bb07329776dccc6590b556.png" alt="14fad07952bb07329776dccc6590b556" style="zoom:75%;" />

**VLSTM 的主要任务**是基于轨迹历史建模代理的角色，通过潜在变量$z_t$ 和隐藏状态 $h_t$提取复杂的时空特征。

**生成模型**负责预测轨迹的下一步状态和奖励，**推断模型**负责从历史中推测潜在变量。

**优化目标**是通过最大化 ELBO 同时优化潜在变量的推断和轨迹的重构。

最终，代理的角色表示由 LSTM 隐藏状态$h_T^{(a)}[n]$ 表示，用于决策分组。

### 分组选择模块

该模块的目的是确定每个agent的群体，即通过选择资源分配的策略网络来确定其对应的行为模式。在该决策问题中，群选择模块需要根据资源分配策略网络的提取角色和智能体的时空行为选择合适的资源分配策略网络。在时间步𝑡= 0处，一旦选择了资源分配策略网络，代理将被绑定到所选的组，直到这个模块结束。

#### **状态（State）**

状态$s^{(g)}$ 表示为：

- $s^{(g)} = \big(I^{(a)}[n], h^{(a)}[n], z^{(g)}[m]\big)$

- $I^{(a)}[n]$：代理 $n$ 的身份标识。
- $h^{(a)}[n]$：从 VLSTM 提取的代理 $n$ 的轨迹角色信息。
- $z^{(g)}[m]$：从 CVAE 提取的群体$m$ 的角色信息。

状态 $s^{(g)}$是一个综合特征，包含了代理的身份、个体角色特征，以及目标群体的角色特征。

#### **动作（Action）**

动作 $a^{(g)}$ 定义为将某个代理$n $分配到某个策略组$m$ 的操作。其计算公式为：

$a^{(g)}[m, n] = \text{Softmax}\Big( \big[ I^{(a)}[n], h^{(a)}[n] \big] W z^{(g)}[m] \Big)$

- **解释**：
  - $a^{(g)}[m, n]$ 表示概率矩阵中第$m$ 行第$n $列的值。
  - 它表示代理 $n $被分配到群体$m $的概率。
  - 使用 Softmax 函数归一化分配概率。
- **公式中的关键项**：
  - $\big[ I^{(a)}[n], h^{(a)}[n] \big]$：代理 $n$ 的特征，包含身份信息$I^{(a)}[n]$和轨迹特征 $h^{(a)}[n]$。
  - $z^{(g)}[m]$：目标群体$m $的角色特征。
  - $W$：权重矩阵，表示模型学习的参数。

通过这个公式，模型将代理的特征与目标群体的特征进行比较，从而计算每个代理属于不同群体的概率。

#### **状态转移（State Transition）**

- 这是一个 **单步 MAMDP**，即群体选择操作在实验开始时执行一次。
- 在整个实验期间，群体分配不再改变，直到实验结束。

#### **奖励（Reward）**

奖励$r^{(g)}$定义为资源分配模块中所有代理的总奖励。具体而言：

- 每个代理根据其被分配的策略组进行资源分配，实验完成后计算总的收益作为奖励。



### 基于组的资源分配模块



