# DYPS

Actor-Critic (AC)方法[24]是MAMDP的典型解决方案

MAMDP问题：多智能体马尔科夫决策过程。

演员——评论家算法：包含两个量：a critic 𝑉𝜋𝜃 and an actor 𝜋𝜃 . 

在训练过程中，通过对当前状态的值进行估计，批评家 𝑉𝜋𝜃起到了基于值的方法的作用。其目的是最小化TD误差𝛿𝑡，以精确估计当前状态的值:

$𝛿𝑡 = (𝛾𝑟(𝑠𝑡, 𝑎𝑡 ) + 𝑉𝜋𝜃 (𝑠𝑡+1) − 𝑉𝜋𝜃 (𝑠𝑡 )^2$.(1)

行动者(actor)通过与环境相互作用并根据当前政策产生动作，从而发挥基于策略方法的作用。它利用了一个优势函数

$𝐴𝜋𝜃(𝑠𝑡, 𝑎𝑡 ) = 𝑟(𝑠𝑡, 𝑎𝑡 ) + 𝑉𝜋𝜃(𝑠𝑡+1) − 𝑉𝜋𝜃(𝑠𝑡 )$,(2)

𝜋𝜃 通过以下式子进行更新：

$∇𝐽(𝜃) = E[∇𝜃𝑙𝑜𝑔𝜋𝜃 (𝑠𝑡, 𝑎𝑡 )𝐴𝜋𝜃(𝑠𝑡, 𝑎𝑡 )]$

## 方法

### 系统概述

图2给出了dyps体系结构的概述。DyPS由三个模块组成:自我监督角色建模、小组选择和基于小组的资源分配。对于时空资源分配问题，智能体表现出相似性和差异性。我们使用VLSTM来捕获单个智能体的特征，并使用CVAE来编码不同组的功能。分组选择模块基于VLSTM和CVAE提取的行为特征，对agent进行动态分组。基于组的资源分配模块包含多个资源分配策略网络，每个网络对应一个特定的资源分配模式。分层决策结构为针对不同资源的决策提供了丰富的行为模式。同时，通过行为模式共享实现参数共享，降低训练成本。

### 自我监督角色建模

为了有效地将代理分组并确定哪些代理共享参数，必须对组和单个代理进行角色建模。我们分别采用两种不同的自监督方法对群体角色和个体代理角色建模，具体如下:

#### CVAE代表群体角色建模

同一组内的代理共享同一策略网络。对每个组的角色进行建模对于有效地聚类代理至关重要。已有研究表明策略网络的行为模式可以通过从agent与环境的交互中得到的状态-动作对来建模。考虑到在某些状态下，不同的政策网络可能做出相同的决策，因此相同的状态-行动对可能对应于几个政策网络。因此，我们使用CVAE对不同策略网络的行为模式进行编码，如图3所示。CVAE同时学习一个概率编码器$q(z^{(g)}[m]I^{(g)}[m];\theta^e)$和一个概率解码器$p(a_t|z^{(g)}[m],s_t;\theta^d)$。式中，$I^{(g)}[m]$是表示对资源分配的识别，策略网络m,$s_t$和$a_t$是资源的状态和策略网络输出的动作，$z^{(g)}[m]$是组m的角色编码。与经典的自编码器结构不同，$s_t$绕过编码器，只能由解码器接收。

基于这种设计，$z^{(𝑔)}[𝑚]$只包含策略本身的信息。我们设置先验$𝑝(z^{(𝑔)}[𝑚];\theta^p)$是标准的多元高斯分布，CVAE的学习就是像VAE一样最大化证据下界ELBO，<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\4128633b544b4907fa7425aeb0a1ba41.png" alt="4128633b544b4907fa7425aeb0a1ba41" style="zoom:75%;" />

#### 用于Agent角色建模的VLSTM

在本节中，我们使用VLSTM从轨迹历史中建模代理的角色。最近的研究[11,20,29]表明，代理人的轨迹包含时空信息。提取这些信息可以帮助组选择代理精确地为代理选择组。因此，我们利用VLSTM[15]从资源的轨迹历史中提取时空特征，将变分自编码器(VAE)[23]与LSTM相结合，增强其在动态环境(如现实世界的网约车)[15]中的鲁棒性[8]。

如图4所示，VLSTM模型包含一个推理模型和一个生成模型。agent𝑛的轨迹历史记为$ T^{(𝑎)}_t[𝑛]= { 𝑠_0，𝑎_0，𝑟_1，𝑠_1，，···，𝑠_{𝑡−1}，𝑎_{𝑡−1}，𝑟_𝑡 }$。在下面的公式中，为简单起见，我们省略了代理识别。例如，我们使用$T_𝑡$代替$T^{(𝑎)}_𝑡[𝑛]$。VLSTM模块可以学习用随机潜变量$z_𝑡$编码复杂的序列特征$T_𝑡$。生成模型$𝑝_{\theta}$预测给定其内部状态$h_{𝑡−1}$的状态-奖励对$𝑒_{𝑡+1} =(𝑠_{𝑡+1}，𝑟_{𝑡+1})$，如图4(c)所示。

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\00dafca35eeaf48a7433b5e13d376d57.png" alt="00dafca35eeaf48a7433b5e13d376d57" style="zoom:75%;" />

其中$𝑓_𝑝$是参数化前馈神经网络，$ℎ_{𝑡−1}$是LSTM的隐藏状态变量，其包含历史时空信息，该历史时空信息可以用该公式递归地更新，如图4(A)所示<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\56411610004c1f2d1d4e771418abde41.png" alt="56411610004c1f2d1d4e771418abde41" style="zoom:75%;" />

其中$𝑓^𝑒$是LSTM层。

如图4(b)所示，VLSTM的推理模型$𝑞$近似于潜变量z𝑡给定状态𝑠𝑡和隐状态变量h𝑡−1

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\14fad07952bb07329776dccc6590b556.png" alt="14fad07952bb07329776dccc6590b556" style="zoom:75%;" />

### 分组选择模块

该模块的目的是确定每个agent的群体，即通过选择资源分配的策略网络来确定其对应的行为模式。在该决策问题中，群选择模块需要根据资源分配策略网络的提取角色和智能体的时空行为选择合适的资源分配策略网络。在时间步𝑡= 0处，一旦选择了资源分配策略网络，代理将被绑定到所选的组，直到这个模块结束。

组选择模块的MAMDP的详细设置如下。

<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\9c5a5b7bd4b0dd432b876c8b4552c870.png" alt="9c5a5b7bd4b0dd432b876c8b4552c870" style="zoom:75%;" />

### 基于组的资源分配模块

该模块由分成多个组的多个代理组成，每个组代表一个资源分配策略网络。 组𝑚的网络参数集表示为𝜓𝑚。然后资源分配模块的参数集为{𝜓1,𝜓2···𝜓𝑀}。 例如，在组选择模块采取操作 𝑎 (𝑔) [𝑚, 𝑛] 并为代理 𝑛 分配组 𝑚 后，该模块向代理 𝑛 提供策略网络 𝜓𝑚。 我们将 𝑎 (𝑎) 𝑡 [𝑛] 表示为代理 𝑛 的动作，它是通过 𝜋(𝑠 (𝑎) 𝑡 [𝑛];𝜓𝑚) 通过策略网络 𝜓𝑚 获得的。 随后，代理使用 Actor-Critic 方法解决时空资源分配问题。<img src="D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-11\Ori\114318c4b90ebdf76f529842c77b6aec.png" alt="114318c4b90ebdf76f529842c77b6aec" style="zoom:75%;" />

### 训练算法

我们在算法1中总结了DyPS的训练过程。正如我们所观察到的，首先，DyPS与环境交互并通过将它们存储在转换集𝐷中来收集一系列转换，为训练做准备（第3-12行）。 然后，这些来自 𝐷 的过渡批次用于训练。  VLSTM 和 CVAE 通过最大化 ELBO 进行训练（第 15-17 行）。 组选择模块和基于组的资源分配模块通过AC方法进行训练（第14行和第18行）。 这个过程将在 𝑀𝑎𝑥𝐸𝑝𝑖 集中重复，直到所有上述 DyPS 模块收敛。