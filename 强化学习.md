# 强化学习

## 学习路径

### 1. **掌握基础 Python 语法**

你应该确保自己已经掌握了以下 Python 基础内容：

- **数据类型与控制结构**：`int`, `float`, `str`, `list`, `tuple`, `dict`, `set`，条件语句（`if`）、循环语句（`for`, `while`）、函数定义与调用、异常处理等。
- **面向对象编程（OOP）**：理解类（`class`）和对象（`object`）的概念，掌握如何定义类和实例化对象。
- **模块与包的使用**：如何导入 Python 的标准库（如 `random`, `os`, `datetime` 等），以及如何使用第三方库。
- **列表与字典的操作**：掌握如何在 Python 中操作和处理列表、字典等容器类型。

如果你还不熟悉这些基础内容，可以参考以下学习资源：

- 《Python 编程：从入门到实践》
- 官方 Python 文档（[Python 官方教程](https://docs.python.org/3/tutorial/)）

### 2. **学习 NumPy 和 PyTorch 基础**

DQN 代码中的深度学习部分主要使用了 **PyTorch**，而训练神经网络的基础是 **NumPy**。所以你需要学习如何使用这两个库。

#### **NumPy 基础**

NumPy 是 Python 中处理数组、矩阵等多维数据的基本库，许多深度学习框架都基于它。你需要理解以下内容：

- 创建和操作多维数组（`ndarray`）
- 数学运算：矩阵运算、广播（broadcasting）等
- 数组的切片、索引、重塑等操作
- 随机数生成（如生成随机数组）

可以参考以下学习资源：

- NumPy 官方文档
- 《Python 科学计算：NumPy 和 SciPy》

#### **PyTorch 基础**

PyTorch 是一个广泛使用的深度学习框架，支持神经网络的构建与训练。你需要掌握以下内容：

- **张量（Tensor）**：PyTorch 中的核心数据结构。张量类似于 NumPy 数组，但可以在 GPU 上运行。
- **神经网络模块（`torch.nn`）**：如何构建神经网络，如何定义层（如 `Linear`, `ReLU`）以及如何进行前向传播。
- **自动求导（Autograd）**：如何计算梯度以及使用反向传播更新权重。
- **优化器（`torch.optim`）**：如何使用不同的优化器（如 Adam、SGD）来更新神经网络的权重。

可以参考以下学习资源：

- PyTorch 官方文档
- Deep Learning with PyTorch: A 60-minute Blitz
- 《Deep Learning with PyTorch》

### 3. **学习强化学习基础**

DQN 是强化学习中的一种算法，因此你需要理解强化学习的基本概念。主要包括以下内容：

- **强化学习的基本要素**：代理（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）等。
- **Q 学习**：强化学习中常见的算法，学习 Q 函数来估计状态-动作对的价值。
- **探索与利用**：`epsilon-greedy` 策略，权衡探索新动作和利用已知最优动作之间的关系。
- **经验回放池**：经验回放池用于存储过去的经验，并从中随机采样进行训练，以打破数据的相关性。

可以参考以下学习资源：

- David Silver's Reinforcement Learning Course
- 《Reinforcement Learning: An Introduction》 by Sutton and Barto（这是一本强化学习的经典教材）

### 4. **逐步构建深度Q网络（DQN）**

当你掌握了基础的 Python 编程、PyTorch 和强化学习之后，你就可以开始实现自己的 DQN 代码了。以下是一步步的指导：

#### **(1) 构建神经网络（`Net` 类）**

首先，定义一个简单的前馈神经网络，用于预测每个动作的 Q 值。你可以使用 PyTorch 的 `nn.Linear` 来定义全连接层，并使用 ReLU 激活函数来引入非线性。

#### **(2) 定义 DQN 类**

DQN 需要两个神经网络：一个是 **评估网络（eval_net）**，用来选择动作并计算 Q 值；另一个是 **目标网络（target_net）**，用来计算目标 Q 值。你可以在 `DQN` 类中使用 `torch.optim.Adam` 来定义优化器，使用 `nn.MSELoss` 来计算损失。

#### **(3) 动作选择（`take_action` 方法）**

根据 epsilon-greedy 策略，你可以在训练时随机选择动作或选择最大 Q 值对应的动作。在测试时，只需选择 Q 值最大的动作。

#### **(4) 网络更新（`update` 方法）**

每次从经验回放池中采样一批数据，使用贝尔曼方程来更新 Q 值。计算损失并通过反向传播更新网络参数。

#### **(5) 经验回放池（Replay Buffer）**

经验回放池用于存储智能体的经验，并提供随机采样的功能。你可以创建一个简单的队列（`collections.deque`），存储状态、动作、奖励和下一个状态的元组。

### 5. **调试与优化**

实现了 DQN 的核心部分后，你需要进行大量的调试和优化：

- **调试训练过程**：在训练过程中，确保网络能够正常收敛，Q 值逐渐逼近目标 Q 值。
- **调整超参数**：如学习率、折扣因子（`gamma`）、epsilon-greedy 策略中的 epsilon 值等，尝试不同的组合以获得最佳的训练效果。

### 6. **进一步学习**

当你熟悉了基础的 DQN 实现后，你可以进一步学习更复杂的强化学习算法，如 Double DQN、Dueling DQN、A3C、PPO 等。

## 第一次学习

### 定义类

使用`class`关键字来定义一个类。类名的首字母通常大写，并采用驼峰命名法（CamelCase）。以下是一个简单的类定义示例：

```python
class MyClass:
    pass  # 使用pass占位，表示这是一个空类
```

### 类属性和方法

#### 类属性

类属性是在类级别定义的变量，它们被类的所有实例共享。

```python
class MyClass:
    class_attribute = "I am a class attribute"
 
# 访问类属性
print(MyClass.class_attribute)  # 输出: I am a class attribute
```

#### 实例属性

实例属性是在类的实例（对象）上定义的变量。它们通常通过初始化方法（`__init__`）来设置。

```python
class MyClass:
    def __init__(self, instance_attribute):
        self.instance_attribute = instance_attribute
 
# 创建类的实例
obj = MyClass("I am an instance attribute")
print(obj.instance_attribute)  # 输出: I am an instance attribute
```

#### 方法

方法是在类内部定义的函数。第一个参数通常是`self`，它代表类的实例本身。

```python
class MyClass:
    def my_method(self):
        print("This is a method")
 
# 创建类的实例并调用方法
obj = MyClass()
obj.my_method()  # 输出: This is a method
```

### 构造函数和析构函数

#### 构造函数

构造函数是在创建类的实例时自动调用的方法。在Python中，通常使用`__init__`作为构造函数。

```python
class MyClass:
    def __init__(self, value):
        self.value = value
 
obj = MyClass(10)
print(obj.value)  # 输出: 10
```

#### 析构函数

析构函数是在类的实例被销毁时自动调用的方法。在Python中，通常使用`__del__`作为析构函数。

```python
class MyClass:
    def __del__(self):
        print("Object is being destroyed")
 
# 创建类的实例，在程序结束时会自动调用析构函数
obj = MyClass()
# 注意：析构函数的输出可能在程序结束时才会看到，取决于垃圾回收的时机
```

### 继承

继承是面向对象编程的一个重要特性，它允许一个类（子类）继承另一个类（父类）的属性和方法。

```python
class Parent:
    def __init__(self, name):
        self.name = name
 
    def greet(self):
        print(f"Hello, my name is {self.name}")
 
class Child(Parent):
    def __init__(self, name, age):
        super().__init__(name)  # 调用父类的构造函数
        self.age = age
 
    def greet(self):
        super().greet()  # 调用父类的方法
        print(f"And I am {self.age} years old")
 
# 创建子类的实例
child = Child("Alice", 12)
child.greet()
# 输出:
# Hello, my name is Alice
# And I am 12 years old
```

### 特殊方法（魔术方法）

Python中有一些特殊方法，它们以双下划线（`__`）开头和结尾，用于实现特定的功能，如字符串表示、数值运算等。

```python
class MyClass:
    def __init__(self, value):
        self.value = value
 
    def __str__(self):
        return f"MyClass with value {self.value}"
 
    def __add__(self, other):
        return MyClass(self.value + other.value)
 
obj1 = MyClass(10)
obj2 = MyClass(20)
print(obj1)  # 输出: MyClass with value 10
print(obj1 + obj2)  # 输出: MyClass with value 30
```

### 静态方法和类方法

#### 静态方法

静态方法不依赖于类的实例，它们可以通过类直接调用，也可以通过实例调用。使用`@staticmethod`装饰器来定义静态方法。

```python
class MyClass:
    @staticmethod
    def static_method():
        print("This is a static method")
 
MyClass.static_method()  # 输出: This is a static method
obj = MyClass()
obj.static_method()  # 输出: This is a static method
```

#### 类方法

类方法依赖于类本身，但不需要实例。它们接收类作为第一个参数（通常是`cls`），而不是实例（`self`）。使用`@classmethod`装饰器来定义类方法。

```python
class MyClass:
    class_variable = "I am a class variable"
 
    @classmethod
    def class_method(cls):
        print(cls.class_variable)
 
MyClass.class_method()  # 输出: I am a class variable
obj = MyClass()
obj.class_method()  # 输出: I am a class variable
```

## 第二次学习

### 创建和操作多维数组（`ndarray`）

**`ndarray`** 是 NumPy 中的核心数据结构，它是一个多维数组（可以是 1D、2D、3D 等），支持高效的数学运算。

创建 `ndarray`

可以通过多种方式创建 `ndarray`：

- **从列表或元组创建：**

  ```python
  import numpy as np
  arr=np.array([1,2,3,4])#一维数组
  print(arr)
  ```

- **创建多维数组：**

  ```python
  arr_2d=np.array([[1,2],[3,4],[5,6]])#二维数组
  print(arr_2d)
  ```

- **使用 `zeros` 和 `ones`：** 创建全为 0 或 1 的数组：

  ```python
  zeros_arr = np.zeros((3,3))#3*3的零矩阵
  ones_arr = np.ones((2,4))#2*4的一矩阵
  ```

- **使用 `arange` 和 `linspace`：** `arange` 创建一个范围内的等差数列，`linspace` 在指定区间内生成均匀分布的点。

  ```python
  arr=np.arange(0,10,2)#[0,2,4,6,8]
  arr_linspace = np.linspace(0,1,5)#[0,0.25,0.5,0.75,1]
  ```

### **数学运算：矩阵运算、广播（broadcasting）等**

NumPy 支持广泛的数学运算，比如矩阵运算、向量化运算（通过广播）等，计算非常高效。

#### 矩阵运算

- **加法、减法、乘法、除法：**

  ```python
  arr1 = np.array([1,2,3])
  arr2 = np.array([4,5,6])
  
  sum_arr = arr1 + arr2
  diff_arr = arr1 - arr2
  prod_arr = arrr1 * arr2
  div_arr = arr1 / arr2
  ```

- **矩阵乘法：**

  ```python
  arr1 = np.array([[1,2],[3,4]])
  arr2 = np.array([[5,6],[7,8]])
  matrix_prod = np.dot(arr1,arr2)
  ```

- **点积：**

  ```python
  vec1 = np.array([1,2,3])
  vec2 = np.array([4,5,6])
  
  dot_product = np.dot(vec1,vec2)
  ```

- **矩阵的转置：**

  ```python
  arr_transposed = arr1.T
  ```

#### 广播（Broadcasting）

广播是 NumPy 中的一种机制，可以在不同形状的数组之间进行运算，自动扩展较小的数组，使其与较大的数组对齐。

- **广播示例：**

  ```python
  arr1 = np.array([1, 2, 3])  # 形状 (3,)
  arr2 = np.array([[4], [5], [6]])  # 形状 (3, 1)
  
  result = arr1 + arr2  # 广播后，arr2 会自动扩展为 (3, 3)
  ```

  结果是：

  ```lua
  [[5 6 7]
   [6 7 8]
   [7 8 9]]
  ```

广播规则：

- 如果数组的维度不同，NumPy 会自动广播小维度数组的形状，使它们匹配。
- 广播时，从最后一维开始，逐维比较。如果维度不同，且其中一个数组的维度为 1，那么可以进行广播扩展。

### **数组的切片、索引、重塑等操作**

#### 切片（Slicing）和索引（Indexing）

- **一维数组切片：**

  ```python
  arr = np.array([1, 2, 3, 4, 5])
  sliced = arr[1:4]  # 提取索引 1 到 3 的元素，结果是 [2, 3, 4]
  ```

- **二维数组切片：**

  ```python
  arr_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
  sliced_2d = arr_2d[1:, :2]  # 从第二行开始，取前两列arr    行，列
  ```

- **高级索引：** 通过传递一个列表或布尔数组进行索引：

  ```python
  arr = np.array([1, 2, 3, 4, 5])
  indices = np.array([0, 2, 4])
  selected = arr[indices]  # 结果是 [1, 3, 5]
  ```

  或者使用布尔数组：

  ```python
  bool_idx = np.array([True, False, True, False, True])
  selected = arr[bool_idx]  # 结果是 [1, 3, 5]
  ```
  
  ### 举例：`grid = order[..., :2].long()`
  
  - **`order[..., :2]`**：
  
    - `order` 是一个多维张量，`...` 是 Python 中的省略符，表示在该位置保持所有维度的原样。
  
    - `:2` 表示取 `order` 中最后一维的前两个元素。假设 `order` 的形状是 `[batch_size, seq_len, num_features]`，那么 `order[..., :2]` 会选取 `num_features` 中的前两个特征。
  
    - 例如，如果 `order` 的形状是 `[2, 3, 5]`（批量大小为 2，序列长度为 3，每个样本包含 5 个特征），`order[..., :2]` 的形状会变成 `[2, 3, 2]`，也就是取每个样本的前两个特征。
  
    - 假设我们有 2 个样本，每个样本有 3 个时间步，每个时间步有 5 个特征。`order` 张量可能是这样的：
  
      ```python
      order = torch.tensor([[[1, 2, 3, 4, 5],
                             [6, 7, 8, 9, 10],
                             [11, 12, 13, 14, 15]],
                            [[16, 17, 18, 19, 20],
                             [21, 22, 23, 24, 25],
                             [26, 27, 28, 29, 30]]])
      ```
  
    -  `contin = state[..., 2:].float()` 是取从第二列开始的所有数据。
    - 如图是我举了一个例子各种切片方法，是行还是列。
    - ![289d1f9d15edea77c793bf816f3fdbf2](D:\QQ\MessageFile\Tencent Files\2752882718\nt_qq\nt_data\Pic\2024-12\Ori\289d1f9d15edea77c793bf816f3fdbf2.png)

#### 重塑（Reshape）

使用 `reshape` 改变数组的形状，但不改变数据：

```python
arr = np.array([1, 2, 3, 4, 5, 6])
reshaped = arr.reshape((2, 3))  # 2 行 3 列的二维数组
```

- `reshape` 的限制:
  - 新形状的维度数和原形状的元素总数必须匹配。

#### 展开（Flatten）

将多维数组转换为一维数组：

```python
arr = np.array([[1, 2], [3, 4]])
flattened = arr.flatten()  # 结果是 [1, 2, 3, 4]
```

------

###  **随机数生成**

NumPy 提供了强大的随机数生成工具，通过 `np.random` 模块生成各种随机数组和分布。

- **生成随机数组：**

  ```python
  rand_arr = np.random.rand(3, 4)  # 生成 3x4 的均匀分布数组，值在 [0, 1) 之间
  ```

- **生成随机整数：**

  ```python
  rand_int = np.random.randint(0,10,size=(3,3))#生成3*3的随机整数矩阵，范围是0-10
  ```

- **正态分布：**

  ```python
  normal_arr = np.random.randn(3,3)#生3*3的标准正态分布(均值0，方差1)
  ```

- **随机选择元素：**

  ```python
  arr = np.array([1, 2, 3, 4, 5])
  choice = np.random.choice(arr, size=3)  # 从数组中随机选择 3 个元素
  ```

## 第三次学习

### 如何构建一个嵌入层？

1. **网格索引是什么？**
   - 网格索引通常指的是一组离散的类别或者数字。例如，在自然语言处理中，词汇表中的每个单词都会被分配一个唯一的索引（比如单词 "apple" 可能对应索引 5，"banana" 对应索引 7，等等）。这些索引是离散的、整数类型的。
   - 在这个上下文中，“网格索引”可能表示类似于某些位置、类别或标识符的离散值。
2. **什么是“嵌入向量”？**
   - 嵌入向量（Embedding vector）是一个连续的、高维的向量，用来表示每个离散的索引。嵌入向量不是随机的，而是通过训练来学习的，目的是将离散的类别（如单词、ID、位置等）映射到一个固定维度的向量空间，能够捕捉到这些类别之间的关系。
   - 比如，给定某个索引 `i`，嵌入层会返回一个向量 `v_i`，这个向量是一个在高维空间中的点（例如一个 64 维或 128 维的向量），可以捕捉到类别 `i` 的某些特征或意义。
3. **将网格索引映射到嵌入向量：**
   - **映射**：这个过程是通过一个嵌入层（`nn.Embedding`）完成的，嵌入层的作用是把每个离散的整数索引映射到一个对应的向量。例如，索引 `3` 可能会被映射到一个 64 维的向量 `[0.12, -0.34, ..., 0.56]`。
   - **为什么要映射**：神经网络通常不能直接处理离散的类别数据。通过嵌入层，我们把这些离散的索引转换成可以计算的、连续的向量，这样网络可以学习这些类别之间的关系。嵌入空间的维度（如 64 维、128 维）是训练过程中会学习的一个超参数。

**举个例子**：

假设你有一个网格索引集合，包含位置（或者类别）信息：

```python

grid = torch.tensor([0, 1, 2, 3])  # 这是一个包含 4 个索引的张量
```

**创建嵌入层**：

```python
embedding_layer = nn.Embedding(4, 3)  # 将 4 个类别映射到 3 维的嵌入空间   3是嵌入维度
```

- **4**：表示有 4 个不同的类别（比如 0, 1, 2, 3）。
- **3**：表示每个类别将会映射到一个 3 维的向量空间（即每个索引会对应一个 3 维向量）。

**映射过程**：

```python
grid_emb = embedding_layer(grid)
print(grid_emb)
```

输出（可能类似于）：

```python
tensor([[-0.3674,  1.2345,  0.5432],  # 对应索引 0 的嵌入向量
        [ 0.8765, -0.1123,  0.3344],  # 对应索引 1 的嵌入向量
        [ 0.2555,  0.8987, -1.2345],  # 对应索引 2 的嵌入向量
        [ 0.1234, -0.2345,  0.5432]]) # 对应索引 3 的嵌入向量
```

**总结**：

- **"网格索引"** 指的是一些离散的数字（比如类目、位置、ID等），
- **"嵌入向量"** 是这些离散数字的高维连续表示，用一个向量来表示每个离散的类别。
- **"映射"** 就是通过 `nn.Embedding` 层把这些离散的整数索引转换为相应的向量，通常这些向量会通过训练学习到一些有意义的特征，反映出类别之间的相似性或者其他关系。

通过这种方式，神经网络可以有效地处理离散的、类别型的数据，而不需要将它们直接作为整数输入模型。

我遇到的代码示例

```python
#一个嵌入层的初始化
self.grid_embedding = layer_init(nn.Embedding(grid_dim, embedding_dim), std=gain, bias_const=0, init=init) 
#嵌入向量生成
grid_emb = self.tanh(self.grid_embedding(grid))
```









### nn.Linear的用法是什么样的？

`nn.Linear` 的构造函数原型是这样的：

```python
nn.Linear(in_features, out_features, bias=True)
```

**参数**：

- **`in_features`**：输入张量的特征数量（即输入的维度）。它是输入张量的最后一个维度的大小。
- **`out_features`**：输出张量的特征数量（即输出的维度）。
- **`bias`**：是否使用偏置项，默认是 `True`。如果为 `True`，那么输出会有一个额外的偏置项；如果为 `False`，则不使用偏置。

**线性层的数学表示**

给定一个输入张量 `X`（形状为 `[batch_size, in_features]`），线性变换的输出是通过以下公式计算的：

$Y = XW^T + b$

- $X$ 是输入张量。
- $W$ 是权重矩阵，形状为 `[out_features, in_features]`。
- $b$ 是偏置向量，形状为 `[out_features]`。
- $Y$ 是输出张量，形状为 `[batch_size, out_features]`。

`nn.Linear(self.contin_dim, embedding_dim)` **解释**

在你提供的代码中：

```python
self.contin_embedding = layer_init(nn.Linear(self.contin_dim,embedding_dim),std=gain,bias_const=0,init=init)
```

这行代码创建了一个线性层，并将其赋值给 `self.contin_embedding`。它将输入的连续特征 `contin`（维度为 `self.contin_dim`）映射到一个新的维度 `embedding_dim`。

**解释：**

- **`self.contin_dim`**：是输入特征的维度，即每个输入样本中包含的连续特征的数量。例如，如果你有一个包含 10 个连续特征的输入样本，`self.contin_dim` 就是 10。
- **`embedding_dim`**：是输出的特征维度，即这个层将连续特征映射到的维度。假设你将输入的连续特征映射到 64 维的空间，那么 `embedding_dim` 就是 64。

**过程：**

- 输入的张量 `contin`，其形状是 `[batch_size, self.contin_dim]`，表示一个批次中每个样本包含 `self.contin_dim` 个连续特征。
- 通过线性变换，`nn.Linear(self.contin_dim, embedding_dim)` 会将每个样本的 `self.contin_dim` 维特征映射到一个 `embedding_dim` 维的向量。
- 如果 `embedding_dim = 64`，那么线性层的权重矩阵 `W` 的形状是 `[64, self.contin_dim]`，偏置项 `b` 的形状是 `[64]`。

**具体例子**：

假设：

- `self.contin_dim = 10`（输入包含 10 个连续特征）。
- `embedding_dim = 64`（输出映射到 64 维空间）。

代码中的线性层会将每个输入样本的 10 个特征映射到一个 64 维的嵌入向量。输入的张量 `contin` 的形状可能是 `[batch_size, 10]`，输出的张量形状则是 `[batch_size, 64]`。

**例子**：

```python
import torch
import torch.nn as nn

# 假设连续特征的维度是 10，嵌入维度是 64
contin_dim = 10
embedding_dim = 64

# 创建线性层
linear_layer = nn.Linear(contin_dim, embedding_dim)

# 创建一个 batch_size = 2 的输入张量，每个样本有 10 个特征
input_tensor = torch.randn(2, contin_dim)

# 通过线性层进行映射
output_tensor = linear_layer(input_tensor)

print(output_tensor.shape)  # 输出: torch.Size([2, 64])
```

**总结**：

- `nn.Linear(self.contin_dim, embedding_dim)` 是一个全连接层，它会将输入张量中形状为 `[batch_size, self.contin_dim]` 的连续特征映射到一个新的空间，输出张量的形状是 `[batch_size, embedding_dim]`。
- 线性层通过一个权重矩阵和一个偏置向量来进行计算，通常用于特征变换、**降维**、特征融合等任务。
- 在训练过程中，`nn.Linear` 的权重和偏置会通过反向传播自动调整，以适应任务的需求。

### 向量拼接

**举个例子**

假设：

- `grid_emb_0` 和 `grid_emb_1` 形状是 `(2, 3, 4)`，代表每个批次有 3 个位置，每个位置 4 维嵌入。
- `contin_emb` 形状是 `(2, 4)`，表示每个批次的一个 4 维连续特征。

```python
# grid_emb_0 (2, 3, 4)
grid_emb_0 = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],
                           [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])

# grid_emb_1 (2, 3, 4)
grid_emb_1 = torch.tensor([[[25, 26, 27, 28], [29, 30, 31, 32], [33, 34, 35, 36]],
                           [[37, 38, 39, 40], [41, 42, 43, 44], [45, 46, 47, 48]]])

# contin_emb (2, 4)
contin_emb = torch.tensor([[50, 51, 52, 53], [54, 55, 56, 57]])

# 拼接
order_emb = torch.cat([grid_emb_0, grid_emb_1, contin_emb.unsqueeze(1).expand(-1, 3, -1)], dim=-1)

print("order_emb:", order_emb)
```

**解释**

- **`contin_emb.unsqueeze(1).expand(-1, 3, -1)`**：
  这里，`contin_emb.unsqueeze(1)` 会将 `contin_emb` 从 `(2, 4)` 转换成 `(2, 1, 4)`，然后通过 `.expand(-1, 3, -1)` 将它扩展到 `(2, 3, 4)`，使它的维度和 `grid_emb_0` 和 `grid_emb_1` 一致。每个批次的连续特征会被复制到每个位置（每个位置都有同样的连续嵌入）。
- **`torch.cat(..., dim=-1)`**：
  在最后一个维度（即嵌入维度）上拼接 `grid_emb_0`、`grid_emb_1` 和扩展后的 `contin_emb`。

**输出**

```python
order_emb: tensor([[[ 1,  2,  3,  4, 25, 26, 27, 28, 50, 51, 52, 53],
                     [ 5,  6,  7,  8, 29, 30, 31, 32, 50, 51, 52, 53],
                     [ 9, 10, 11, 12, 33, 34, 35, 36, 50, 51, 52, 53]],

                    [[13, 14, 15, 16, 37, 38, 39, 40, 54, 55, 56, 57],
                     [17, 18, 19, 20, 41, 42, 43, 44, 54, 55, 56, 57],
                     [21, 22, 23, 24, 45, 46, 47, 48, 54, 55, 56, 57]]])
```

**总结**

- **`torch.cat([...], dim=-1)`**：
  沿着最后一个维度（即嵌入维度）进行拼接。
- **`contin_emb`**：
  `contin_emb` 在最后拼接时，被扩展为每个位置相同的连续嵌入，放在拼接的最后，形成新的 12 维嵌入向量。

拼接后的 `order_emb` 张量的形状是 `(2, 3, 12)`，表示每个批次有 3 个位置，每个位置有 12 维嵌入向量。

我遇到的代码是这样的：

```python
order_emb = torch.cat([grid_emb[..., 0, :], grid_emb[..., 1, :], contin_emb], dim=-1)
```

### 为什么要进行这样的操作？

1. **信息融合：**
   通过 `torch.cat([grid_emb[..., 0, :], grid_emb[..., 1, :], contin_emb], dim=-1)`，将 `grid_emb` 中的第 0 行、第 1 行和 `contin_emb` 结合在一起。这一步的目的是将不同来源的特征信息融合到一个嵌入空间中，为后续的网络层提供更丰富的信息。
2. **非线性变换：**
   `self.tanh(self.order_layer2(order_emb))` 通过使用 `tanh` 激活函数引入非线性变换。非线性变换能够帮助网络更好地捕捉复杂的关系，使得模型有更强的表达能力，而不是仅仅依赖于线性变换。
3. **进一步映射：**
   `self.order_layer3(order_emb)` 通过进一步的线性变换（或其他类型的变换），将融合后的特征映射到最终需要的空间。这一步通常是为了得到最终任务的输出，比如预测、分类或其他任务的结果。

### 如何构造一个RNN网络？

使用GRU（门控循环单元）来实现循环神经网络（RNN）层。

```python
class RNNLayer(nn.Module):
    def __init__(self, input_dim, output_dim, layer_num, init=True):
        super(RNNLayer, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.layer_num = layer_num
        self.init = init

        self.rnn = nn.GRU(input_dim, output_dim, num_layers=layer_num)
        # self.norm = nn.LayerNorm(outputs_dim)
        if self.init:
            for name, param in self.rnn.named_parameters():
                if 'bias' in name:
                    nn.init.constant_(param, 0)
                elif 'weight' in name:
                    nn.init.orthogonal_(param)

    def forward(self, input, hidden):
        '''
        input: (seq length, batch size, feature dim)
        hidden:(layer num,  batch size, hidden dim)
        output:(seq length, batch size, output dim)
        '''
        output, hidden = self.rnn(input.unsqueeze(0), hidden)
        return output.squeeze(0), hidden
```

其中`nn.GRU` 是 PyTorch 中用于实现门控循环单元（GRU，Gated Recurrent Unit）的类，GRU 是一种常用的循环神经网络（RNN）变体，用于处理序列数据。

`nn.GRU` **构造函数**

```python
torch.nn.GRU(input_size,hidden_size,num_layers=1,bias=True，batch_first=False, dropout=0,bidirectional=False)
```

**参数解释：**

- `input_size`：输入特征的维度，即每个时间步的输入大小（例如，如果输入是词嵌入，`input_size` 是词嵌入的维度）。
- `hidden_size`：隐藏层状态的维度（每个时间步的隐状态大小）。GRU 会输出这个大小的隐状态。
- `num_layers`：GRU 网络的层数，默认值是 1。通常可以设置为多个层以加深网络。
- `bias`：是否使用偏置项，默认值为 `True`。
- `batch_first`：如果为 `True`，则输入和输出的张量的形状将是 `(batch_size, seq_len, input_size)`。如果为 `False`，则形状为 `(seq_len, batch_size, input_size)`。默认值是 `False`。
- `dropout`：在非最后一层之间是否添加 Dropout，默认值为 0（即不使用 Dropout）。
- `bidirectional`：是否使用双向 GRU。双向 GRU 会从正序和逆序两个方向处理输入序列，默认值是 `False`。

**输入**：`input` 是形状为 `(seq_len, batch_size, input_size)` 的张量，表示序列数据。`seq_len` 是序列长度，`batch_size` 是批次大小，`input_size` 是每个时间步的特征数量。如果 `batch_first=True`，输入的形状是 `(batch_size, seq_len, input_size)`。

**输出**：

- `output`：形状为 `(seq_len, batch_size, hidden_size)` 的张量（如果 `batch_first=False`），包含每个时间步的隐藏状态。如果是双向 GRU，则输出会有两个方向的隐藏状态。
- `h_n`：形状为 `(num_layers * num_directions, batch_size, hidden_size)` 的张量，表示最后一个时间步的隐藏状态。`num_directions` 是 2（对于双向 GRU），否则为 1。

**权重初始化**

如果 `init` 参数为 `True`，则对 RNN 的参数进行初始化：

```python
if self.init:
    for name, param in self.rnn.named_parameters():
        if 'bias' in name:
            nn.init.constant_(param, 0)#常量初始化
        elif 'weight' in name:
            nn.init.orthogonal_(param)#正交初始化
            #orthogonal_ 和 constant_ 是 PyTorch 中用于初始化张量（如模型的权重和偏置）的两个函数。

```

- 对 `bias`（偏置项）进行常数初始化，值为 0。
- 对 `weight`（权重矩阵）进行正交初始化。正交初始化有助于防止梯度消失或爆炸问题。

**前向传播**`forward`

```python
def forward(self, input, hidden):
    output, hidden = self.rnn(input.unsqueeze(0), hidden)
    return output.squeeze(0), hidden
```

在前向传播中，`input.unsqueeze(0)` 是在输入的第一个维度（时间步维度）添加一个维度。通常 GRU 接收的输入是 `(seq_len, batch_size, input_dim)`，但有时候需要调整形状来符合 `GRU` 接口要求。然后将输入和隐层状态传递给 `GRU` 层。

### History类

这段代码定义了一个继承自 `nn.Module` 的 PyTorch 神经网络模型，模型的名称是 `history`。它主要包含一个 LSTM 层和一个全连接的线性层，并且使用了额外的输入（`obs`）来进行预测。

```python
class history(nn.Module):
    def __init__(self, input_size=81,obs_size=65,hidden_layer_size=200, output_size=1):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size

        self.lstm = nn.LSTM(input_size, hidden_layer_size,batch_first=True)

        self.linear = nn.Linear(hidden_layer_size*2, output_size)

        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size))
        self.obs=nn.Linear(obs_size, hidden_layer_size)

    def forward(self, input_seq,obs=None):
        input_seq=input_seq.view(-1,144,size_lstm)
        lstm_out, (h,c) = self.lstm(input_seq)
        h = h.squeeze(0)
        if obs==None:
            return h.detach().cpu()
        else:

            h=h.view(-1,grid_num,self.hidden_layer_size)
            obs_=F.relu(self.obs(obs))
            predictions=torch.cat([obs_,h],-1)
            predictions = self.linear(predictions)
            return predictions
```

1. `__init__` **方法**

- 这个初始化方法定义了模型的结构，`input_size` 是输入数据的特征数，`obs_size` 是观察数据的特征数，`hidden_layer_size` 是 LSTM 隐藏层的大小，`output_size` 是输出层的大小。

2. **LSTM** 层

- 创建一个 LSTM 层，输入数据的大小为 `input_size`，输出的隐藏状态维度是 `hidden_layer_size`。`batch_first=True` 表示输入数据的形状是 `(batch_size, seq_len, input_size)`。如果为 `False`，则形状为 `(seq_len, batch_size, input_size)`。默认值是 `False`。

3. **线性层**

- 定义了一个全连接层，将大小为 `hidden_layer_size * 2` 的输入映射到 `output_size`。为什么是 `* 2`？因为后面会将 LSTM 的输出和 `obs` 输入拼接在一起。

4. **初始化隐藏状态**

- 初始化 LSTM 的隐藏状态 `h` 和单元状态 `c`，这里都初始化为全零。
- **隐藏状态和单元状态** 是 LSTM 用来存储当前时间步的“记忆”信息，不是权重。它们在网络的前向传播中不断更新。

6. `forward` **方法**

- 这个方法定义了前向传播的逻辑。

- `input_seq.view(-1, 144, size_lstm)` 重新调整输入序列的形状，`144` 是序列的长度，`size_lstm` 可能是一个预定义的常量，表示每个时间步的输入特征数。

- 然后将输入传入 LSTM 层，得到 `lstm_out`（LSTM 输出的所有时间步的隐状态）和 `(h, c)`（LSTM 的最后一层隐状态和单元状态）。

- `h.squeeze(0)` 去掉多余的维度，使 `h` 的形状为 `(batch_size, hidden_layer_size)`。

7. **处理观察数据**

```python
if obs == None:
    return h.detach().cpu()
else:
    h = h.view(-1, grid_num, self.hidden_layer_size)#h.view() 是 PyTorch 中一个用于 改变张量形状 的函数。它的作用是将一个张量重新调整为指定的形状（即维度）。
    #-1 表示自动推导这个维度的大小，以保证总元素数量不变。
    #grid_num 和 self.hidden_layer_size 是目标形状的其它两个维度。
    obs_ = F.relu(self.obs(obs))#F 是 torch.nn.functional 的别名
    #这个模块包含了很多不需要创建模型参数（如权重或偏置）的函数，例如激活函数、损失函数、卷积操作、池化操作等。
    predictions = torch.cat([obs_, h], -1)
    predictions = self.linear(predictions)
    return predictions
```

- 如果没有提供 `obs` 数据（即 `obs == None`），那么返回 LSTM 的最后一个隐藏状态 `h`，并将其从计算图中分离出来，移到 CPU 上。
- 如果提供了 `obs` 数据：
  1. 将 `h` 重新形状为 `(batch_size, grid_num, hidden_layer_size)`，`grid_num` 是一个预定义的常量，可能是与数据相关的维度。
  2. 使用 ReLU 激活函数处理 `obs` 数据，并将结果传入线性层 `self.obs`。
  3. 使用 `torch.cat` 将处理后的观察数据和 LSTM 的输出拼接成一个新的张量。
  4. 将拼接后的张量传入 `self.linear` 进行最终的预测，并返回预测结果。

**总结**

- 这个模型的输入是一个序列数据（`input_seq`），通过 LSTM 处理后得到隐藏状态。

- 如果提供了额外的观察数据（`obs`），则将其与 LSTM 的输出拼接，然后通过一个线性层进行最终的预测。

- 如果没有提供观察数据，只返回 LSTM 的最后一个隐藏状态。

### 图神经网络自注意力机制（**Graph Attention Network (GAT)** ）

