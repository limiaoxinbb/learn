# 10.23组会论文

## 优化方面

这项研究提出了一个马尔可夫决策过程，以联合优化电动汽车车队的充电和订单调度方案，该车队仅从指定的交通枢纽（即，没有从不同的位置拾取）。目标是在有限的时间内使船队的总收入最大化。电动汽车车队的完整状态转移方程制定跟踪其电池的荷电状态。为了在动态随机环境下学习充电和订单调度策略，提出了一种在线近似算法，该算法将基于模型的强化学习（RL）框架与一种新的SARSA（）-样本平均近似（SAA）结构相结合.与无模型RL算法和近似动态规划（ADP）算法相比，该算法通过具有经验状态转移的SAA模型探索高质量决策，并通过SARSA（）样本轨迹更新探索迄今为止的最佳决策。基于一个真实的案例的计算结果表明，与现有的启发式方法和文献中的ADP相比，该方法的日收益平均分别提高了31.76%和14.22%.

## 方法

在每个时段开始时，C&D系统做出联合计费和调度决策（即，一个战术计划）在当前时间段内命令随机到达之前。战术计划（Crainic，20 0 0; Rehgaya & Vanderbeck，2007; Quetschlich等人，2021）确定具有不同SOC水平的EV的数量以服务即将到来的订单（即，调度决定）以及要充电的EV的数量（即，收费决定）从该期间开始。战术计划不确定特定的EV来服务特定的订单或调度EV从枢纽到乘客目的地的路线。它是在平台业务一级对动态和实时匹配和路由选择模块的一项投入。在本研究中，我们使用启发式优先级规则来有效匹配可用的电动汽车和到达订单。在周期结束时，系统观察特定电动汽车满足的订单和电动汽车车队的更新状态，然后为下一个周期做出新的充电和调度决策。综上所述，在该问题中，C&D系统需要在每个周期开始时基于所有EV的观测状态（即，在集散中心可用、被收费或服务订单）。注意，对于每个前期订单，对应的消耗电力和行驶时间被假设为已知参数，其可以由给定的实时车辆路线安排方法外部地确定，如文献中广泛研究的（例如，Florio等，2023; Zhang等人，2023年）的报告。

## 问题描述

C&D系统中的优化问题的形式描述如下。按需电子叫车平台管理一个EV车队，表示为一个集合E，每天运行。操作日由n索引并被划分为T个时段（即，时隙）具有相同的时间长度。在一天的非操作时段期间（例如，午夜12时），所有电动车将不会提供订单，并将被充电至第二天的全功率水平。如图1所示，任何电动汽车都可以处于以下状态之一：（i）在交通枢纽可用，称为可用电动汽车;（ii）服务订单（包括返程），称为在用电动汽车;（iii）由于等待或在充电站充电而停止服务，称为停止服务电动汽车。此外，我们假设电动汽车的电池容量是同质的，他们的电池SOC同样离散到K + 1水平。具有SOC水平k ∈ { 0，1，...，K}的EV被简称为k型EV。特别是，k = 0意味着EV的SOC低于阈值并且无法服务任何订单，k = K意味着EV的电池充满电。

类似地，订单可以根据其所请求的SOC被分类为K种类型。..，K}，简称为类型1订单，意味着EV需要消耗1级SOC来完成订单并返回到集线器。

我们使用向量D t = {D l t| 1 ≤ l ≤ K}来表示在时间段t期间宣布的订单的集合，其中D l t是类型1订单的数量。根据我们的调查，DALLT是随时间变化的，并且在很大程度上取决于枢纽的运营时间表。此外，由于复杂多变的环境和较高的计算成本，用F t（Δ D l t）表示的概率分布函数（PDF）难以准确估计。

在这项研究中，完成任何订单的旅行时间，包括往返枢纽的时间，被假定为已知和估计的（Al-Kanj等人，2020年; Hua等人，2019年; Yu等人，2021年）通过先进的实时交通信息和导航系统，如谷歌地图和AMap。此外，为了模型的简单性，我们假设存在一个线性关系之间的旅行时间和相应的消耗SOC水平。也就是说，行程时间是ρ × l，其中ρ是测量一个SOC水平可以支持的行程时间段的给定参数。此外，通过假设固定的充电功率，k型EV完全充电的充电时间为c ×（K-k），其中c是对一个SOC水平进行充电的周期数。

![image-20241021104957705](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241021104957705.png)

在这项研究中，完成任何订单的旅行时间，包括往返枢纽的时间，被假定为已知和估计的（Al-Kanj等人，2020年; Hua等人，2019年; Yu等人，2021年）通过先进的实时交通信息和导航系统，如谷歌地图和AMap。此外，为了模型的简单性，我们假设存在一个线性关系之间的旅行时间和相应的消耗SOC水平。也就是说，行程时间是ρ × l，其中ρ是测量一个SOC水平可以支持的行程时间段的给定参数。此外，通过假设固定的充电功率，k型EV完全充电的充电时间为c ×（K-k），其中c是对一个SOC水平进行充电的周期数。

![image-20241021105121736](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241021105121736.png)

如前所述，在每个周期t的开始，t ∈ { 1，2，...，T }，C&D系统做出联合决策Z t = { X t，Y t }，其中包括计费决策X t = { x k t| 0 ≤ k ≤ K}和一个调度决策Yt = {yk，lt| 1 ≤ l ≤ k ≤ K}，然后在周期t内，订单D t随机到达。具体地，xk，t  表示要被再充电到满功率水平的可用类型k EV的数量，并且yk，lt   指定在当前时段期间服务即将到来的类型  l  订单的可用类型k EV的数量，其中所服务的订单的类型可以是l = 1，2，...， k。在周期t结束时，实际满足的第l类订单数为min（K k = l y k，l t，？D l t），因此获得的收益为β（l）× min（K k = l y k，l t，？D l t）。如果在时间段t期间，任何EV都不为类型1订单提供服务，则平台将遭受惩罚成本θ（l）。这里，β（l）和θ（l）被公式化为消耗的SOC水平l的函数，其考虑了到枢纽的空载返回的成本，而不管EV是否在其往返行程中接载客户。此外，单台电动汽车的充电费用由γ（k，t）计算，γ（k，t）取决于该电动汽车当前的功率水平k，并且当电动汽车充满电时，从起始时段t ≥ t到结束时段t + c（K − k）的实时电价，即γ（k，t）= t + c（K−k）i = t ei，其中ei为第i时段的充电费用。

![image-20241021105138582](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241021105138582.png)

关于时段t的电动汽车充电和调度的联合决策Z t不仅影响当前时段的收入，而且影响SOC水平以及未来时段的电动汽车可用性。在本节的其余部分，我们将问题建模为随机MDP，它由五个部分组成：状态，决策空间，外生信息，状态转换和奖励函数。

## 不理解的地方

Q1：此外，我们假设电动汽车的电池容量是同质的，他们的电池SOC同样离散到K + 1水平。具有SOC水平k ∈ { 0，1，...，K}的EV被简称为k型EV。特别地，k = 0意味着EV的SOC低于阈值并且不能服务于任何订单，并且k = K意味着EV具有完全充电的电池。是什么意思？我不是很理解，难道说不同的车具有不同的SOC水平？比如有的车是1型EV，有的是5型EV？

A1：不同的电动汽车在不同时间点可能处于不同的SOC水平。例如：

- 一辆电动汽车处于SOC = 1，称为“1型EV”，表示它的电量非常少，但还能提供少量服务。
- 另一辆电动汽车SOC = 5，称为“5型EV”，说明电量充足，能够提供更多的服务。

每个类型的EV都可以根据它的SOC来决定是否能够接单或者接多少订单。这个划分帮助模型更好地管理和优化电动汽车的调度和充电策略。

## 重点

SARSA (State-Action-Reward-State-Action) 是强化学习（Reinforcement Learning, RL）中的一种常用算法，属于基于策略的时序差分（on-policy Temporal Difference, TD）学习算法。SARSA算法通过与环境交互来学习策略，在每一步时序更新时使用的状态和动作都是从当前策略中选取的。

SARSA架构包含以下核心要素：

1. **状态 (State, S)**：代表环境的当前情况。
2. **动作 (Action, A)**：智能体在给定状态下执行的操作。
3. **奖励 (Reward, R)**：智能体执行某一动作后获得的反馈信号，通常是一个标量，用来衡量该动作的好坏。
4. **下一个状态 (Next State, S')**：智能体执行动作后到达的新的状态。
5. **下一个动作 (Next Action, A')**：在新的状态S'下根据当前策略选择的动作。

### SARSA算法流程：

1. 初始化Q表（Q值表）并设置学习率、折扣因子等参数。
2. 在状态 $S$ 下选择动作 $A$（通常使用 $ϵ$-贪婪策略选择）。
3. 执行动作 $A$，得到奖励 $R$并进入下一个状态 $S'$。
4. 在状态 $S'$ 下选择下一个动作 $A$。
5. 更新Q值：使用以下更新规则：$Q(S,A)←Q(S,A)+α[R+γQ(S',A')−Q(S,A)]$
6. 其中：
   - $α $是学习率
   - $γ$ 是折扣因子，用来衡量未来奖励的重要性

7.将 $S \leftarrow S'$， $A \leftarrow A'$，重复以上过程，直到达到终止条件。

SARSA的一个特点是它属于**基于策略的学习算法**，它直接依据当前的策略进行学习和更新。与Q-learning（无策略算法）相比，SARSA是用当前策略评估未来动作的价值，这意味着它更“保守”，不如Q-learning那样激进。

这个架构常用于解决马尔科夫决策过程（MDP）问题，比如在机器人路径规划、游戏AI等领域中。

### SAA优化

SAA优化通常指的是**样本平均逼近法**（Sample Average Approximation, SAA），是一种用于解决随机优化问题的数值方法。SAA方法通过将随机优化问题中的不确定性用样本来逼近，从而将原来的随机优化问题转化为确定性问题。

### SAA优化的核心思想：

在随机优化问题中，目标函数通常是一个期望值，形式为：

$\min_x \mathbb{E}_\xi [f(x, \xi)]$

其中， $x$ 是决策变量，$ξ$ 是一个随机变量，$f(x,ξ)$ 是给定决策变量 $x$ 和随机变量 $ξ$ 后的目标函数值。由于 $ξ$ 是随机的，直接优化该期望值通常很难。SAA方法通过生成随机样本 $ξ_1,ξ_2,…,ξ_N $来逼近期望值，将问题转化为以下确定性问题：

$\min_x \frac{1}{N} \sum_{i=1}^{N} f(x, \xi_i)$

其中，$N$ 是样本数量。通过这种方式，原来的随机优化问题变成了一个确定的优化问题，这样可以使用常规的优化算法来求解。

### SAA优化的步骤：

1. **生成样本**：从随机变量 $ξ$ 的分布中生成多个样本 $ξ1,ξ2,…,ξN$。
2. **构建样本平均问题**：用生成的样本逼近期望值，构建一个确定性优化问题。
3. **求解确定性问题**：使用常见的优化算法（如梯度下降法、线性规划等）求解样本平均问题。
4. **估计误差**：根据样本的数量和解的稳定性，评估样本平均逼近的效果以及解的误差。

### SAA的优势：

- **可计算性**：将难以求解的随机优化问题转化为确定性问题，能够应用现有的优化技术来求解。
- **渐近最优性**：当样本数量 $N$ 足够大时，SAA问题的解可以逼近原问题的最优解。
- **灵活性**：SAA适用于各种形式的随机优化问题，尤其是在实际问题中数据不确定的情境下，如供应链优化、投资组合优化等领域。

### 适用场景：

SAA优化常用于解决包含不确定性或随机性的复杂优化问题，尤其是在以下场景中：

- **金融领域**：如投资组合优化、风险管理等问题，其中未来收益是不确定的。
- **供应链管理**：如库存管理和运输优化，供应和需求存在随机性。
- **能源调度**：如电力系统中的负荷和发电量不确定的优化问题。

DQN强化学习？