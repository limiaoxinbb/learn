# UniST

## 优势

UniST通过以下方式取得成功：

（i）利用来自不同场景的多样化时空数据，

（ii）有效的预训练以捕获复杂的时空动态，

（iii）知识引导的提示以增强泛化能力。这些设计共同释放了为各种场景构建通用模型的潜力。

在本文中，我们的目标是建立通用城市时空预测的基础模型，具体来说，开发一种通用模型，在不同的时空场景中提供卓越的性能和强大的泛化能力。

## 预训练和提示学习

### 阶段一

第一阶段：大规模时空预训练。 与仅限于单个数据集的现有方法不同，我们的方法利用来自各个领域和城市的广泛时空数据进行预训练。

### 阶段二

第二阶段：时空知识引导的提示学习。 我们引入了一种用于上下文学习的提示网络，其中提示的生成是由完善的时空域知识（例如空间层次结构和时间周期性）自适应引导的。

## 基础模型

我们的基本模型是一个基于transformer的编码器-解码器架构。通过时空修补，可以统一的顺序格式处理各种时空数据。

### 时空分块

![image-20241105172506385](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241105172506385.png)

### 位置编码

由于原始 Transformer 架构不考虑序列的顺序，因此我们遵循结合位置编码的常见做法 [9]。 为了增强泛化能力，我们选择正弦和余弦函数而不是可学习参数来进行位置编码。 该编码分别应用于空间和时间维度。

### 编码器-解码器结构

基本模型采用受 Masked Autoencoder (MAE) [18] 启发的编码器解码器框架。 它以一定的掩蔽比处理输入补丁，其中编码器获取未掩蔽的补丁，解码器使用编码器的输出和掩蔽的补丁重建图像。 我们的重点是捕获全面的时空依赖性，包括高层和低层关系，目标是准确预测特定时间和空间坐标的值。 与使用轻量级解码器进行预训练的 MAE 不同，我们的模型采用全尺寸解码器，在预训练和微调中都发挥着至关重要的作用。

![image-20241105173418433](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241105173418433.png)

## 时空自监督预训练

- 随机掩蔽
- 管掩蔽
- 块掩蔽
- 时间掩蔽

通过采用这些多样化的掩蔽策略，模型可以从综合角度系统地增强其建模能力，同时解决时空、空间和时间关系。

## 时空知识引导提示

![image-20241105174144925](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241105174144925.png)



![image-20241105174840172](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241105174840172.png)

## 时空领域知识

需要强调的是，𝐸𝑠𝑐、𝐸𝑠ℎ、𝐸𝑡𝑐、𝐸𝑡𝑝的学习不受我们修行的限制。 从业者可以灵活地采用更复杂的设计来捕获更丰富的时空属性。 例如，基于傅立叶的方法 [38, 60] 可用于捕获周期性模式。



![image-20241105181140674](C:\Users\27528\AppData\Roaming\Typora\typora-user-images\image-20241105181140674.png)

左侧展示了输入数据的多种时空分块方式，如随机、块状和管状分块。

中间部分展示了 Stage 1 的预训练过程，使用 Transformer 进行时空特征的学习。

右侧的 Stage 2 展示了知识引导的提示学习，通过提示网络以及空间和时间的记忆池来优化时空数据的理解。