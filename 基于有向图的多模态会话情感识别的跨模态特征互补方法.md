# 基于有向图的多模态会话情感识别的跨模态特征互补方法

现有方法的不足：然而，这些方法直接连接多模态信息，而不考虑模态之间的相互作用。此外，基于递归的方法倾向于使用最近的话语进行建模，这使得这些模型难以收集远距离信息。

## 本文贡献：

- 我们提出了一个基于有向图的跨模态特征互补(GraphCFC)模块。GraphCFC既能有效缓解多模态融合的异质性差距问题，又能充分提取多模态对话图中的多样性信息。

- 设计了一种新的GNN层GAT-MLP，不仅缓解了GNN的过平滑问题，而且为多模态学习提供了新的网络框架。
- 将会话表示为具有可变上下文话语的多模态有向图，并从该图中提取不同类型的边进行编码，使GAT-MLP能够准确地选择关键的上下文信息和交互信息。
- 在两个基准数据集上进行了广泛的对比实验和消融研究。实验结果表明，我们提出的GraphCFC能够有效地互补和融合多模态特征，与以前的SOTA方法相比，获得了最佳的性能。

## 本文方法介绍：

鉴于上述多模态情感识别的挑战，我们在本节中介绍了一种新的基于图的ERC多模态特征融合方法。该部分包括概述、单模态编码器、基于图的跨模态特征互补(GraphCFC)模块和多模态情感分类器四部分。

### 概述：

![](E:\meet\多模态融合\图神经网络\GraphCFC1.png)

![](E:\meet\多模态融合\图神经网络\GraphCFC2.png)

### 模型总体架构：

图2为本文基于图的多模态ERC总体架构，主要包括单模态编码、基于图的跨模态特征互补(GraphCFC)和多模态情感分类。首先，我们利用三个单模态编码器对单模态特征进行编码。其次，采用基于GNN的跨模态特征互补模块采集远距离模态内上下文信息和模态间交互信息。最后，我们利用多个损失函数建立了多任务学习模型，用于多模态情绪分类。

![](E:\meet\多模态融合\图神经网络\GraphCFC3.png)

### 单模态编码

为了获取文本模态的上下文感知特征信息(参考MMGCN[5])，我们利用了双向长短期记忆(BiLSTM)网络。文本情态的特征预提取可表述为:

![公式1](E:\meet\多模态融合\图神经网络\4.png)

![公式二](E:\meet\多模态融合\图神经网络\5.png)

### 基于图的跨模态特征互补模块

该模块包括两类信息互补，即模态内语境信息和模态间交互信息。

GraphCFC模块分为五个主要部分。

- 首先，我们描述了如何构造图;
- 其次，引入多个子空间映射，同时保证多模态特征的一致性和多样性;
- 第三，提出了一种新的图神经网络结构GAT-MLP;
- 第四，引入基于GAT-MLP的成对跨模态互补(PairCC)，缓解了多模态ERC的异质性差距问题;多模态异质性差距（multimodal heterogeneity gap）指的是在多模态数据（如文本、音频和视觉）中，不同模态之间的特征表示、数据结构和信息表达方式存在的差异性。这种差异可能导致在多模态融合过程中信息难以直接对齐和融合，从而影响模型的性能。
- 最后，详细介绍了本文所使用的GAT- mlp的GAT结构。

#### 图的构造：

![](E:\meet\多模态融合\图神经网络\3-1.png)

2.多模态图的构造

假设存在两种模态P和Q

![](E:\meet\多模态融合\图神经网络\3-2.png)

**在图中，边被定义为节点之间的连接。在多模态对话图中，我们从两个角度定义边缘:模态内话语的语境连接和多模态话语的互动连接。特别地，我们将这两种类型的边分别称为边内(Eintra)和边间(Einter)。内边缘用于捕获模态内上下文信息，而间边缘用于捕获跨模态交互信息**。

![](E:\meet\多模态融合\图神经网络\3-3.png)

#### 多子空间提取器：

**受MMGCN b的启发，我们认为说话人的信息很重要。多说话人$S_{emb}$的嵌入可以形式化为:**

$S_{emb}=Embedding(S,D)$

式中，S为说话人的集合，D为说话人的数量。为了对说话人身份信息进行编码，我们将说话人嵌入到话语节点的特征中:

$X^ζ_{spk} = µS_{emb} +X^ζ$

式中，$X^ζ (ζ∈{t, a, v})$为单模编码器的特征矩阵，$X^ζ_i$∈$X^ζ$;$X^ζ_{spk}$表示添加说话人嵌入的特征矩阵;µ∈[0,1]为说话人嵌入的比值。

目前，我们在多模态融合中遇到的一个难题是异质性间隙[14]的存在。换句话说，数据在各个模态之间的分布是不一致的。因此，在进行跨模态特征互补之前，我们将每个模态的特征映射到一个共享的子空间中，以保持跨模态特征表示的一致性。然而，多模态的特征表征越相似，模态之间的特征互补性越差。换句话说，我们希望保持多种模态特征表示的多样性，这样一种模态的特征就可以补充其他模态的特征。鉴于此，我们将每个模态的特征映射到单独的子空间中，以捕获跨模态的特征表示的多样性。同时获取多模态的多样性和一致性信息有助于模态之间的互补和融合。

在多模态情感识别任务中，为了在共享子空间中捕捉多模态信息的一致性和差异性，如何使用映射函数进行特征变换。

先是单模态内的操作：

![](E:\meet\多模态融合\图神经网络\4-1.png)

提取单个模态下的一致性特征矩阵。

![](E:\meet\多模态融合\图神经网络\4-2.png)

将三种模态的一致性特征矩阵拼接起来，然后做一个线性变换。

**什么是独立子空间和共享子空间？**

![](E:\meet\多模态融合\图神经网络\4-3.png)

**那么映射函数是什么呢？**

![](E:\meet\多模态融合\图神经网络\4-4.png)

![](E:\meet\多模态融合\图神经网络\4-5.png)

损失函数的计算

![](E:\meet\多模态融合\图神经网络\4-6.png)

#### GAT-MLP层:

我们基于ResNet和Transformer的思想设计了一个新的GNN层GAT-MLP。GAT-MLP层可表述为:

![](E:\meet\多模态融合\图神经网络\4-7.png)

Xin是一个多模态特征矩阵。

#### 基于GAT-MLP的PairCC:

如果将多模态的特征直接拼接在一起，不仅会因异质性差距而难以融合，而且会忽略跨模态的交互信息。更糟糕的是，模态数量越多，模态之间的异质性差距问题就越严重。因此，我们提出了基于GAT-MLP的成对跨模态互补(PairCC)策略，用于跨模态特征交互并最小化异质性差距。基于GAT-MLP的PairCC过程如图图2所示，该过程主要由GAT-MLP和级联层组成。具体而言，我们首先将视觉和声学模态特征矩阵输入GAT-MLP层进行模态内上下文和模态间交互编码，并将视觉和声学编码结果拼接得到v-a(视觉-声学)特征矩阵$H^{va}$;然后将$H^{va}$作为一种新模态的特征矩阵，在$H^{va}$与文本特征矩阵之间进行相同的编码操作，得到v-a-t(视觉-声学-文本)特征矩阵$H^{vat}$;最后，对$H^{vat}$和共享子空间的特征矩阵进行类似编码，得到最终的特征矩阵。以上步骤可简单表述如下:

![](E:\meet\多模态融合\图神经网络\4-8.png)

#### SingleGAT:

![](E:\meet\多模态融合\图神经网络\4-9.png)

在注意力机制中，$\mathbf{a}^\top$ 通常作为一个权重向量，用于衡量不同特征维度对注意力分数的影响。

接下来详细说明了$α_{ij}$和$α'_{ij}$的计算方法，后一个是加入了边类型权重。

![](E:\meet\多模态融合\图神经网络\5-1.png)

![](E:\meet\多模态融合\图神经网络\5-2.png)

### 多模态情绪分类器

![](E:\meet\多模态融合\图神经网络\5-3.png)

最后是总损失函数。